{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed Cross Validated Parameter Search\n",
    "------------------------------------\n",
    "\n",
    "In the previous section we parallelized cross-validated parameter search on a single machine.  In this notebook we do the same exercise, but now on a distributed cluster.  \n",
    "\n",
    "### Requirements\n",
    "\n",
    "This notebook should be run on the provided cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "We train a machine learning model across many parameters with cross validation.  This is slightly more complex than a map so we use `submit`.  We train a support vector classifier on handwritten digits using cross validation to avoid over-fitting.\n",
    "\n",
    "As before we start with a sequential solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import ParameterSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cv_params_demo import load_cv_split, evaluate_one  # Functions we care about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Software Environment\n",
    "\n",
    "We will run into issues if our worker machines lack the `cv_params_demo.py` file holding some of the custom functions necessary for this section.  Distributed computing frameworks have mechanisms to solve this by sending .py files around.  In order to skip dealing with this we're just going to include all of the content of that file in this notebook.  \n",
    "\n",
    "Run the following cell twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load cv_params_demo.py\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.externals.joblib import hash\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_cv_split(split_idx):\n",
    "    data = load_digits()\n",
    "    splitted = train_test_split(data.data, data.target,\n",
    "                                test_size=0.20,\n",
    "                                random_state=split_idx)\n",
    "    return split_idx, splitted\n",
    "\n",
    "def evaluate_one(model_class, parameters, cv_split):\n",
    "    split_idx, (X_train, X_val, y_train, y_val) = cv_split\n",
    "    model = model_class(**parameters).fit(X_train, y_train)\n",
    "\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    validation_score = model.score(X_val, y_val)\n",
    "\n",
    "    results = {\n",
    "        'train_score': train_score,\n",
    "        'val_score': validation_score,\n",
    "        'parameters': parameters,\n",
    "        'parameters_hash': hash(parameters),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_param_map(df, target, title):\n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    plt.xlabel('log10(C)')\n",
    "    plt.ylabel('log10(gamma)')\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.scatter(np.log10(df['C']), np.log10(df['gamma']),\n",
    "                c=target,\n",
    "                marker='s', edgecolors='none',\n",
    "                s=80, alpha=1, cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    return fig\n",
    "\n",
    "def plot_results(results):\n",
    "    results = pd.DataFrame.from_dict(results)\n",
    "\n",
    "    mean_evaluations = results.groupby('parameters_hash').agg({\n",
    "        'val_score': np.mean,\n",
    "       # 'training_time': np.mean,\n",
    "    }).reset_index()\n",
    "\n",
    "    all_parameters = pd.DataFrame.from_dict(list(results['parameters']))\n",
    "    all_parameters['parameters_hash'] = results['parameters_hash']\n",
    "\n",
    "    evaluations = (\n",
    "        mean_evaluations\n",
    "        .merge(all_parameters)\n",
    "        .drop(['parameters_hash'], axis=1)\n",
    "    )\n",
    "    top10 = evaluations.sort_values(\n",
    "        by='val_score', ascending=False).head(10)\n",
    "\n",
    "    fig = plot_param_map(evaluations, evaluations['val_score'],\n",
    "                   'validation score')\n",
    "    plt.scatter(np.log10(top10['C']), np.log10(top10['gamma']),\n",
    "                c='r', s=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "digits = load_digits()  # Collect Data\n",
    "\n",
    "plt.imshow(digits.data[0].reshape(8, 8),  # Example element\n",
    "           interpolation='nearest', cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': np.logspace(-10, 10, 1001),\n",
    "    'gamma': np.logspace(-10, 10, 1001),\n",
    "    'tol': np.logspace(-4, -1, 4),\n",
    "}\n",
    "\n",
    "param_space = ParameterSampler(param_grid, 10)\n",
    "\n",
    "list(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cv_params_demo import load_cv_split\n",
    "\n",
    "cv_splits = [load_cv_split(i) for i in range(2)]\n",
    "idx, (x_train, x_test, y_train, y_test) = cv_splits[0]\n",
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential cross validated parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "\n",
    "for split in cv_splits:\n",
    "    for params in param_space:\n",
    "        result = evaluate_one(SVC, params, split)\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results\n",
    "\n",
    "Which regions of parameter space score well?  Can we tell from the results we've computed?  \n",
    "\n",
    "Searching over more parameters would help to improve the intuition we can gain here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cv_params_demo import plot_results\n",
    "\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Distributed parallel cross validated parameter search\n",
    "\n",
    "We will use both Spark or Dask.distributed to scale our computation across multiple machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_splits = [load_cv_split(i) for i in range(2)]  # Increase the number 2 after parallel computation acheived\n",
    "param_space = ParameterSampler(param_grid, 10)    # Increase the number 10 after parallel computation acheived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrent.futures solution\n",
    "\n",
    "We load the solution using `concurrent.futures`.  Then we replace the stdlib `concurrent.futures.ThreadPoolExecutor` with an API compatible executor from either `ipyparallel` or `dask.distributed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/cvgs-1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Solution\n",
    "\n",
    "We load the single-machine solution using the local Spark instance `'local[4]'`.  We replace this SparkContext with a new SparkContext pointing to the cluster instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/cvgs-2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
