{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel Collections\n",
    "---------------------\n",
    "\n",
    "Systems like Spark and Dask include \"big data\" collections with a small set of high-level primitives.  With these common patterns we can often handle computations that are more complex than map, but still structured.\n",
    "\n",
    "In this section we repeat the SKLearn example using the PySpark RDD and the Dask Bag, which both provide parallel operations on linear collections of arbitrary objects.\n",
    "\n",
    "\n",
    "### Objectives\n",
    "\n",
    "*  Use the `concurrent.futures` function `submit` to parallelize non-map patterns\n",
    "\n",
    "### Requirements\n",
    "\n",
    "*  SciKit Learn\n",
    "*  PySpark\n",
    "*  Dask.bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "We train a machine learning model across many parameters with cross validation.  This is slightly more complex than a map so we use `submit`.  We train a support vector classifier on handwritten digits using cross validation to avoid over-fitting.\n",
    "\n",
    "As before we start with a sequential solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import ParameterSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cv_params_demo import load_cv_split, evaluate_one\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "plt.imshow(digits.data[0].reshape(8, 8),\n",
    "           interpolation='nearest', cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': np.logspace(-10, 10, 1001),\n",
    "    'gamma': np.logspace(-10, 10, 1001),\n",
    "    'tol': np.logspace(-4, -1, 4),\n",
    "}\n",
    "\n",
    "param_space = ParameterSampler(param_grid, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cv_params_demo import load_cv_split\n",
    "\n",
    "cv_splits = [load_cv_split(i) for i in range(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local[4]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_rdd = sc.parallelize(param_space)\n",
    "cv_rdd = sc.parallelize(cv_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = param_rdd.cartesian(cv_rdd).map(lambda ab: evaluate_one(SVC, ab[0], ab[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask.bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "param_bag = db.from_sequence(param_space)\n",
    "cv_bag = db.from_sequence(cv_splits)\n",
    "\n",
    "b = param_bag.product(cv_bag).map(lambda a, b: evaluate_one(SVC, a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "results = b.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import dask\n",
    "results = b.compute(get=dask.threaded.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "*  Higher level collections include functions for common patterns\n",
    "*  Move data to collection, construct lazy computation, trigger at the end\n",
    "*  Used PySpark (`cartesian + map`) and Dask.bag (`product + map`) to handle nested for loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
